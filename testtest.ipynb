{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Transformer 모델 정의 (이전 설명에 따라 정의된 모델을 사용)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 예: Transformer 모델 클래스 정의 부분 생략\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# model = Transformer(...)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 학습 설정\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mpad_token_id)\n\u001b[1;32m     10\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[1;32m     11\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Transformer 모델 정의 (이전 설명에 따라 정의된 모델을 사용)\n",
    "# 예: Transformer 모델 클래스 정의 부분 생략\n",
    "# model = Transformer(...)\n",
    "\n",
    "# 학습 설정\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for src_batch, tgt_batch in train_loader:\n",
    "        src = src_batch.to(device)\n",
    "        tgt = tgt_batch.to(device)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "\n",
    "        src_mask = None  # 실제로는 패딩 마스크 등을 사용\n",
    "        tgt_mask = None  # 실제로는 시퀀스 마스크 등을 사용\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "        loss = criterion(output.view(-1, tgt_vocab_size), tgt_output.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {avg_train_loss}\")\n",
    "\n",
    "    # 검증\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src_batch, tgt_batch in val_loader:\n",
    "            src = src_batch.to(device)\n",
    "            tgt = tgt_batch.to(device)\n",
    "\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "\n",
    "            output = model(src, tgt_input)\n",
    "            loss = criterion(output.view(-1, tgt_vocab_size), tgt_output.view(-1))\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch + 1}, Validation Loss: {avg_val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(model, tokenizer, text, max_length=150):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "    summary_ids = model.generate(input_ids, max_length=max_length)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# 예제 사용법\n",
    "text = \"Your input article text here.\"\n",
    "summary = summarize_text(model, tokenizer, text)\n",
    "print(\"Summary:\", summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
